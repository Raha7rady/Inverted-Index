#include <iostream>
#include <fstream>
#include <string>
#include <vector>
#include <unordered_map>
#include <algorithm>
#include <cmath>
#include <iomanip>
#include <cctype>

using namespace std;

/* ============================================================
   Tokenization Utilities
   ------------------------------------------------------------
   This function converts raw text into a sequence of normalized
   tokens by:
   - Removing non-alphanumeric characters
   - Converting all characters to lowercase
   ============================================================ */
vector<string> tokenize(const string& text)
{
    vector<string> tokens;
    string currentToken;

    for (unsigned char ch : text) {
        if (isalnum(ch)) {
            currentToken += static_cast<char>(tolower(ch));
        }
        else if (!currentToken.empty()) {
            tokens.push_back(currentToken);
            currentToken.clear();
        }
    }

    // Add the last token if it exists
    if (!currentToken.empty())
        tokens.push_back(currentToken);

    return tokens;
}

/* ============================================================
   Core Data Structures
   ------------------------------------------------------------
   DocID          : Unique identifier for each document
   TermFrequency  : Number of occurrences of a term in a document
   PostingList    : Maps DocID -> TermFrequency
   InvertedIndex  : Maps term -> PostingList
   ============================================================ */
using DocID = int;
using TermFrequency = int;

using PostingList = unordered_map<DocID, TermFrequency>;
using InvertedIndex = unordered_map<string, PostingList>;

/* ============================================================
   TF-IDF Vector Space Ranking
   ------------------------------------------------------------
   Computes relevance scores for each document using:
     score(doc) = Î£ ( TF(term, doc) * IDF(term) )

   Parameters:
   - query      : Tokenized query terms
   - index      : Inverted index built from the document corpus
   - totalDocs  : Total number of documents in the corpus

   Returns:
   - A vector of (DocID, score) pairs sorted by descending score
   ============================================================ */
vector<pair<DocID, double>> rankDocuments(
    const vector<string>& query,
    const InvertedIndex& index,
    int totalDocs)
{
    unordered_map<DocID, double> documentScores;

    for (const auto& term : query) {
        auto termIt = index.find(term);
        if (termIt == index.end())
            continue;  // Skip query terms not present in the corpus

        const PostingList& postings = termIt->second;
        double idf = log(static_cast<double>(totalDocs) / postings.size());

        for (const auto& [docID, tf] : postings) {
            documentScores[docID] += tf * idf;
        }
    }

    // Ensure that all documents appear in the final ranking
    for (DocID id = 1; id <= totalDocs; ++id) {
        documentScores.try_emplace(id, 0.0);
    }

    vector<pair<DocID, double>> rankedResults(
        documentScores.begin(), documentScores.end());

    sort(rankedResults.begin(), rankedResults.end(),
        [](const auto& a, const auto& b) {
            return a.second > b.second;
        });

    return rankedResults;
}

/* ============================================================
   Document Loader & Index Builder
   ------------------------------------------------------------
   Reads text files, tokenizes their contents, and builds
   an inverted index.

   Parameters:
   - files : List of file paths
   - index : Reference to the inverted index to be populated

   Returns:
   - Number of successfully loaded documents
   ============================================================ */
int buildInvertedIndex(const vector<string>& files, InvertedIndex& index)
{
    int documentsLoaded = 0;

    for (size_t i = 0; i < files.size(); ++i) {
        ifstream inputFile(files[i]);
        if (!inputFile) {
            cerr << "Error: Unable to open file -> " << files[i] << endl;
            continue;
        }

        ++documentsLoaded;
        DocID docID = static_cast<DocID>(i + 1);
        string line;

        while (getline(inputFile, line)) {
            vector<string> tokens = tokenize(line);
            for (const auto& token : tokens) {
                index[token][docID]++;
            }
        }
    }

    return documentsLoaded;
}

/* ============================================================
   Main Application Entry Point
   ============================================================ */
int main()
{
    // List of document paths (corpus)
    vector<string> documentFiles = {
        "Document1.txt",
        "Document2.txt",
        "Document3.txt"
    };

    InvertedIndex index;

    int totalDocuments = buildInvertedIndex(documentFiles, index);

    if (totalDocuments == 0) {
        cerr << "Error: No documents were successfully loaded.\n";
        return EXIT_FAILURE;
    }

    // Example query (already normalized)
    vector<string> query = { "information", "retrieval" };

    auto results = rankDocuments(query, index, totalDocuments);

cout << "\n==================================================\n";
cout << " Information Retrieval Results\n";
cout << "==================================================\n";

cout << "Query: \"";
for (size_t i = 0; i < query.size(); ++i) {
    cout << query[i];
    if (i + 1 < query.size()) cout << " ";
}
cout << "\"\n";

cout << "Total Documents: " << totalDocuments << "\n\n";

cout << left
     << setw(6)  << "Rank"
     << setw(14) << "Document ID"
     << setw(18) << "Relevance Score" << '\n';

cout << string(38, '-') << '\n';

int rank = 1;
for (const auto& [docID, score] : results) {
    cout << left
         << setw(6)  << rank++
         << setw(14) << ("Doc " + to_string(docID))
         << fixed << setprecision(3)
         << score << '\n';
}

cout << "==================================================\n";

    return EXIT_SUCCESS;
}
